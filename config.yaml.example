# AutoWriterLLM Configuration Example
# Copy this file to config.yaml and update with your settings

# Default Provider
provider: "anthropic"  # The provider to use by default

# Provider Configurations
providers:
  # Anthropic Configuration
  anthropic:
    api_key: "your_anthropic_api_key_here"
    base_url: "https://api.anthropic.com"
    models:
      - name: "claude-3-opus-20240229"
        description: "Most powerful Claude model with advanced reasoning"
        max_tokens: 8192
        temperature: 0.7
      - name: "claude-3-sonnet-20240229"
        description: "Balanced Claude model for most tasks"
        max_tokens: 8192
        temperature: 0.7
      - name: "claude-3-haiku-20240307"
        description: "Fast and efficient Claude model"
        max_tokens: 4096
        temperature: 0.7
  
  # Lingyiwanwu Configuration
  lingyiwanwu:
    api_key: "your_lingyiwanwu_api_key_here"
    base_url: "https://api.lingyiwanwu.com/v1"
    models:
      - name: "yi-lightning"
        description: "Latest high-performance model"
        max_tokens: 8192
        temperature: 0.7
      - name: "yi-large"
        description: "Large model with enhanced capabilities"
        max_tokens: 8192
        temperature: 0.7
      - name: "yi-medium"
        description: "Medium-sized balanced model"
        max_tokens: 4096
        temperature: 0.7
      - name: "yi-vision"
        description: "Complex vision task model"
        max_tokens: 4096
        temperature: 0.7
      - name: "yi-medium-200k"
        description: "200K context window model"
        max_tokens: 8192
        temperature: 0.7
      - name: "yi-spark"
        description: "Small but efficient model"
        max_tokens: 4096
        temperature: 0.7
      - name: "yi-large-rag"
        description: "Real-time web retrieval model"
        max_tokens: 8192
        temperature: 0.7
      - name: "yi-large-fc"
        description: "Function calling support"
        max_tokens: 8192
        temperature: 0.7
      - name: "yi-large-turbo"
        description: "High performance-cost ratio"
        max_tokens: 8192
        temperature: 0.7
  
  # OpenAI Configuration
  openai:
    api_key: "your_openai_api_key_here"
    base_url: "https://api.openai.com/v1"
    models:
      - name: "gpt-4o"
        description: "Latest GPT-4 Omni model"
        max_tokens: 8192
        temperature: 0.7
      - name: "gpt-4-turbo"
        description: "Fast GPT-4 model"
        max_tokens: 4096
        temperature: 0.7
      - name: "gpt-3.5-turbo"
        description: "Efficient GPT-3.5 model"
        max_tokens: 4096
        temperature: 0.7
  
  # Custom Provider Template (copy and modify for additional providers)
  # custom_provider:
  #   api_key: "your_api_key_here"
  #   base_url: "https://api.custom-provider.com/v1"
  #   models:
  #     - name: "model-name"
  #       description: "Model description"
  #       max_tokens: 4096
  #       temperature: 0.7

# Content Generation Options
# These options control how content is generated
multi_part_generation: true
parts_per_section: 5  # Number of parts to generate for each section
max_tokens_per_part: 8192  # Maximum tokens per part (will be overridden by model-specific settings if available)

# Context Management Options
# These options control how context from previous sections is used
include_learning_metadata: true  # Include learning objectives and prerequisites
include_previous_chapter_context: true  # Include context from previous chapters
include_previous_section_context: true  # Include context from previous sections

# Output Options
output_dir: "output"  # Directory for generated content
cache_dir: "cache"  # Directory for cached content

# Rate Limiting
rate_limit:
  enabled: true
  requests_per_minute: 10  # Maximum API requests per minute

# Vector Database Options
vector_db:
  enabled: true
  collection_name: "book_content"
  embedding_model: "all-MiniLM-L6-v2"

# Logging Options
logging:
  level: "INFO"  # Options: "DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"
  file: "logs/autowriterllm.log"
  max_size_mb: 10
  backup_count: 5

# API Configuration
api:
  anthropic:
    api_key: "your-anthropic-api-key-here"  # Replace with your actual API key
    model: "claude-3-sonnet-20240229"       # Specify Claude model version
    max_tokens: 4096                        # Maximum tokens per request
    temperature: 0.7                        # Controls randomness (0.0-1.0)

# Output Configuration
output:
  directory: "output"                       # Directory for generated content
  format: "markdown"                        # Output format (markdown/rst)

# Logging Configuration
logging:
  level: "INFO"                            # Logging level (DEBUG/INFO/WARNING/ERROR)
  file: "logs/autowriter.log"              # Log file location
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Content Generation Settings
generation:
  max_chapters: 15                         # Maximum number of chapters
  include_exercises: true                  # Include practice exercises
  include_examples: true                   # Include code examples
  language: "en"                           # Content language

# Template Settings
templates:
  directory: "templates"                   # Directory containing templates
  toc_template: "toc_template.md"         # Table of contents template
  chapter_template: "chapter_template.md"  # Chapter template 